{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "import sklearn\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from pandas.plotting import scatter_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'date', 'hour', 'bc_price', 'bc_demand', 'ab_price', 'ab_demand',\n",
      "       'transfer', 'bc_price_evo'],\n",
      "      dtype='object')\n",
      "id                int64\n",
      "date            float64\n",
      "hour            float64\n",
      "bc_price        float64\n",
      "bc_demand       float64\n",
      "ab_price        float64\n",
      "ab_demand       float64\n",
      "transfer        float64\n",
      "bc_price_evo     object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "#On constate qu'il y'a 9 neufs colonnes\n",
    "df = pd.read_csv(\"train.csv\")\n",
    "print(df.columns)\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quelques plots "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import scatter_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "scatter_matrix(df,figsize=(10,10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Valeurs aberrantes dans la colonne ab_price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "On supprime les doublons, on supprime les valeurs nulles \n",
    "On supprime aussi la colonne id qui n'est pas pertinent pour notre modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_non_duplicates= df.drop_duplicates()\n",
    "df_non_missing = df_non_duplicates.dropna()\n",
    "df_non_aberrantes = df_non_missing[df_non_missing[\"ab_price\"]<0.1]\n",
    "df_non_aberrantes = df_non_aberrantes.loc[~((df_non_aberrantes[\"bc_price\"] < 0.1) & (df_non_aberrantes[\"ab_price\"] > 0.01))]\n",
    "target_name = \"bc_price_evo\"\n",
    "target, data = df_non_aberrantes[target_name] , df_non_aberrantes.drop(columns=[target_name,\"id\",\"transfer\",\"ab_price\",\"ab_demand\"])   #On sépare la colonne target des autres colonnes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import scatter_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "scatter_matrix(data,figsize=(10,10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Très important KNeighborsClassifier se base sur le calcul des distances,\n",
    "#donc un prétraitement des variables rend ce modèle plus efficace\n",
    "model = Pipeline(steps=[\n",
    "    (\"classifier\",RandomForestClassifier(n_estimators = 200,criterion = \"entropy\", max_features=6, max_depth=80)),])\n",
    "\n",
    "#Liste des hyparamètres\n",
    "all_preprocessors = [\n",
    "    None,\n",
    "    StandardScaler(),\n",
    "    MinMaxScaler(),\n",
    "    QuantileTransformer(n_quantiles=100),\n",
    "    PowerTransformer(method=\"box-cox\"),\n",
    "]\n",
    "\n",
    "param_grid ={\"classifier__n_estimators\": [100,200,300],\n",
    "             \"classifier__max_features\" : [6,8,10],\n",
    "             \"classifier__max_depth\": [80, 90, 100, 110]\n",
    "             }\n",
    "#Fin de la partie des hyparamètres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.88135358, 0.88045212])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#deux(02) validations croisées internes pour déterminer les meilleurs hyparamètres\n",
    "model_grid_search = GridSearchCV(model, param_grid=param_grid,n_jobs=2, cv=2)\n",
    "\n",
    "#dix(10) validations croisées externes pour déterminer les performences du modèles\n",
    "scores = cross_validate(\n",
    "    model_grid_search, data, target, cv=2, n_jobs=2, return_estimator=True\n",
    ")\n",
    "\n",
    "\n",
    "cv_test_scores = scores['test_score']\n",
    "cv_test_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.86210723, 0.86438679])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "clf = AdaBoostClassifier(estimator =GradientBoostingClassifier(n_estimators=200, learning_rate=1.0,max_depth=1, random_state=0) ,n_estimators=100)\n",
    "scores = cross_validate(\n",
    "    clf, data, target, cv=2, n_jobs=2, return_estimator=True\n",
    ")\n",
    "cv_test_scores = scores['test_score']\n",
    "cv_test_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.89856078, 0.89682677, 0.90272239, 0.89628859, 0.89611516])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "clf = AdaBoostClassifier(estimator =RandomForestClassifier(n_estimators = 500, criterion='entropy',max_features=5,max_depth=200),n_estimators=200)\n",
    "scores = cross_validate(\n",
    "    clf, data, target, cv=5, n_jobs=2, return_estimator=True\n",
    ")\n",
    "cv_test_scores = scores['test_score']\n",
    "cv_test_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.84961885, 0.85654886, 0.85377685, 0.86417186, 0.86451836,\n",
       "       0.86204506, 0.85788562, 0.85753899, 0.8592721 , 0.85337955])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "clf3 = HistGradientBoostingClassifier().fit(data, target)\n",
    "scores = cross_validate(\n",
    "    clf3, data, target, cv=10, n_jobs=2, return_estimator=True\n",
    ")\n",
    "cv_test_scores = scores['test_score']\n",
    "cv_test_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Initialiser les apprenants faibles\n",
    "tree_weak_learner = DecisionTreeClassifier()\n",
    "\n",
    "clf = GradientBoostingClassifier(init=DecisionTreeClassifier(), n_estimators=200, max_depth=3,learning_rate=0.5,subsample=1.0,random_state=42).fit(data, target)\n",
    "#max_depth=1\n",
    "score = clf.score(data,target)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.83033918, 0.84517203])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = cross_validate(\n",
    "    clf, data, target, cv=2, n_jobs=2, return_estimator=True\n",
    ")\n",
    "cv_test_scores = scores['test_score']\n",
    "cv_test_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = model.fit(data, target)\n",
    "data_test = pd.read_csv(\"test.csv\")\n",
    "column_id = data_test[\"id\"]\n",
    "data_test = data_test.drop(columns=\"id\")\n",
    "# Faire des prédictions avec le modèle KNN\n",
    "predictions = knn.predict(data_test)\n",
    "# Créer un DataFrame avec les colonnes \"id\" et \"predictions\"\n",
    "result_df = pd.DataFrame({'id': column_id, 'predictions': predictions})\n",
    "result_df.to_csv(\"predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Précision du test: 0.8358690001732801\n"
     ]
    }
   ],
   "source": [
    "knn2 =  model.fit(data, target)\n",
    "#predictions = knn2.predict(data)\n",
    "accuracy = knn2.score(data, target)\n",
    "print(\"Précision du test:\",accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Précision du test: 0.7238055613341654\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "svm_model_linear = SVC(kernel = 'linear').fit(data, target)\n",
    "accuracy = svm_model_linear.score(data, target)\n",
    "print(\"Précision du test:\",accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Précision du test: 1.0\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_estimators = 300, criterion='entropy',max_features=8, max_depth=100) \n",
    "knn3 =  clf.fit(data, target)\n",
    "#predictions = knn2.predict(data)\n",
    "accuracy = knn3.score(data, target)\n",
    "print(\"Précision du test:\",accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable date : 0.2787476240998431\n",
      "Variable hour : 0.07802831419308959\n",
      "Variable bc_price : 0.3635902734662574\n",
      "Variable bc_demand : 0.12882290940035684\n",
      "Variable ab_price : 0.044662904072290434\n",
      "Variable ab_demand : 0.052739367090951186\n",
      "Variable transfer : 0.053408607677211464\n"
     ]
    }
   ],
   "source": [
    "# Obtenir l'importance des variables\n",
    "feature_importances = clf.feature_importances_\n",
    "\n",
    "# Afficher l'importance des variables\n",
    "for i, importance in enumerate(feature_importances):\n",
    "    print(f\"Variable {data.columns[i]} : {importance}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.89647997, 0.89613317, 0.90462979, 0.89698231, 0.8997572 ])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_estimators = 300, criterion='entropy',max_features=8,max_depth=200) \n",
    "scores = cross_validate(\n",
    "    clf, data, target, cv=5, n_jobs=2, return_estimator=True\n",
    ")\n",
    "cv_test_scores = scores['test_score']\n",
    "cv_test_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rd_forest =  AdaBoostClassifier(estimator =RandomForestClassifier(n_estimators = 300, criterion='entropy',max_features=5,max_depth=200),n_estimators=50).fit(data, target)\n",
    "data_test = pd.read_csv(\"test.csv\")\n",
    "column_id = data_test[\"id\"]\n",
    "data_test = data_test.drop(columns=[\"id\",\"transfer\",\"ab_price\",\"ab_demand\"])\n",
    "\n",
    "# Faire des prédictions avec le modèle random forest\n",
    "predictions = model_rd_forest.predict(data_test)\n",
    "# Créer un DataFrame avec les colonnes \"id\" et \"predictions\"\n",
    "result_df = pd.DataFrame({'id': column_id, 'predictions': predictions})\n",
    "result_df.to_csv(\"predictions.csv\", index=False)\n",
    "#RandomForestClassifier(n_estimators = 300, criterion='entropy',max_features=5,max_depth=200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = pd.read_csv(\"test.csv\")\n",
    "column_id = data_test[\"id\"]\n",
    "data_test = data_test.drop(columns=[\"id\",\"ab_demand\",\"transfer\"])\n",
    "# Faire des prédictions avec le modèle KNN\n",
    "predictions = knn3.predict(data_test)\n",
    "# Créer un DataFrame avec les colonnes \"id\" et \"predictions\"\n",
    "result_df = pd.DataFrame({'id': column_id, 'predictions': predictions})\n",
    "result_df.to_csv(\"predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = pd.read_csv(\"test.csv\")\n",
    "scatter_matrix(data_test,figsize=(10,10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meilleurs hyperparamètres pour la validation croisée #1:\n",
      "{'classifier__max_depth': 100, 'classifier__max_features': 8, 'classifier__n_estimators': 300}\n",
      "Meilleurs hyperparamètres pour la validation croisée #2:\n",
      "{'classifier__max_depth': 90, 'classifier__max_features': 10, 'classifier__n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "#Liste des meilleurs hyparamètres pour chaque validation croisée\n",
    "for cv_fold, estimator_in_fold in enumerate(scores[\"estimator\"]):\n",
    "    print(\n",
    "        f\"Meilleurs hyperparamètres pour la validation croisée #{cv_fold + 1}:\\n\"\n",
    "        f\"{estimator_in_fold.best_params_}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
